{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processamento de Linguagem Natural com spaCy\n",
    "\n",
    "Este notebook cobre os tópicos básicos de Processamento de Linguagem Natural (PLN) utilizando a biblioteca spaCy. Incluímos exemplos para:\n",
    "- Tokenização\n",
    "- Stopwords\n",
    "- Stemming (substituído por Lematização no spaCy)\n",
    "- Lematização\n",
    "- POS-Tagging\n",
    "- Reconhecimento de Entidades Nomeadas (NER)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Instalação do spaCy e modelo em português\n",
    "\n",
    "Antes de começar, certifique-se de que o spaCy e o modelo em português estão instalados. Execute o seguinte comando no terminal:\n",
    "\n",
    "```bash\n",
    "!pip install spacy\n",
    "!python -m spacy download pt_core_news_sm\n",
    "```\n",
    "\n",
    "Isso instalará o spaCy e o modelo de linguagem em português."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Carregar o modelo de português"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('pt_core_news_sm')  # Carregar o modelo de português"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tokenização\n",
    "\n",
    "Tokenização é o processo de dividir o texto em unidades menores chamadas *tokens*. Esses tokens podem ser palavras ou frases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Texto de exemplo\n",
    "texto = \"O gato está dormindo no sofá. Ele é muito preguiçoso.\"\n",
    "doc = nlp(texto)\n",
    "\n",
    "# Tokenização por palavras\n",
    "tokens_palavras = [token.text for token in doc]\n",
    "print(\"Tokenização por palavras:\", tokens_palavras)\n",
    "\n",
    "# Tokenização por sentenças\n",
    "tokens_sentenças = [sent.text for sent in doc.sents]\n",
    "print(\"Tokenização por sentenças:\", tokens_sentenças)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Stopwords\n",
    "\n",
    "Stopwords são palavras comuns que são geralmente removidas em análises de texto, pois não contribuem para o significado principal (ex: \"o\", \"e\", \"de\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remover stopwords\n",
    "tokens_sem_stopwords = [token.text for token in doc if not token.is_stop]\n",
    "print(\"Texto sem stopwords:\", tokens_sem_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Lematização\n",
    "\n",
    "A lematização é o processo de transformar uma palavra em sua forma base (lema). O spaCy usa lematização por padrão, pois oferece melhores resultados do que o stemming em muitos casos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lematização das palavras no texto\n",
    "lematizadas = [token.lemma_ for token in doc]\n",
    "print(\"Palavras lematizadas:\", lematizadas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. POS-Tagging (Part-of-Speech Tagging)\n",
    "\n",
    "POS-Tagging é o processo de etiquetar cada palavra em uma sentença com sua respectiva categoria gramatical, como substantivo, verbo, adjetivo, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar POS-Tagging nas palavras\n",
    "tags = [(token.text, token.pos_) for token in doc]\n",
    "print(\"POS-Tagging:\", tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. NER (Reconhecimento de Entidades Nomeadas)\n",
    "\n",
    "O NER é o processo de identificar entidades nomeadas, como pessoas, locais, organizações, datas, etc., dentro de um texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar NER no texto\n",
    "entidades = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "print(\"Entidades reconhecidas:\", entidades)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

