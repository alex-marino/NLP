{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processamento de Linguagem Natural com spaCy\n",
    "\n",
    "Este notebook cobre os tópicos básicos de Processamento de Linguagem Natural (PLN) utilizando a biblioteca spaCy. Incluímos exemplos para:\n",
    "- Tokenização\n",
    "- Stopwords\n",
    "- Stemming (substituído por Lematização no spaCy)\n",
    "- Lematização\n",
    "- POS-Tagging\n",
    "- Reconhecimento de Entidades Nomeadas (NER)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Instalação do spaCy e modelo em português\n",
    "\n",
    "Antes de começar, certifique-se de que o spaCy e o modelo em português estão instalados. Execute o seguinte comando no terminal:\n",
    "\n",
    "```bash\n",
    "!pip install spacy\n",
    "!python -m spacy download pt_core_news_sm\n",
    "```\n",
    "\n",
    "Isso instalará o spaCy e o modelo de linguagem em português."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Carregar o modelo de português"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T20:04:35.709412Z",
     "start_time": "2024-09-12T20:04:35.308123Z"
    }
   },
   "source": [
    "# !python -m spacy download pt_core_news_sm\n",
    "import spacy\n",
    "nlp = spacy.load('pt_core_news_sm')  # Carregar o modelo de português"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tokenização\n",
    "\n",
    "Tokenização é o processo de dividir o texto em unidades menores chamadas *tokens*. Esses tokens podem ser palavras ou frases."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T20:04:43.555751Z",
     "start_time": "2024-09-12T20:04:43.543051Z"
    }
   },
   "source": [
    "# Texto de exemplo\n",
    "texto = \"O gato está dormindo no sofá. Ele é muito preguiçoso.\"\n",
    "doc = nlp(texto)\n",
    "\n",
    "# Tokenização por palavras\n",
    "tokens_palavras = [token.text for token in doc]\n",
    "print(\"Tokenização por palavras:\", tokens_palavras)\n",
    "\n",
    "# Tokenização por sentenças\n",
    "tokens_sentenças = [sent.text for sent in doc.sents]\n",
    "print(\"Tokenização por sentenças:\", tokens_sentenças)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenização por palavras: ['O', 'gato', 'está', 'dormindo', 'no', 'sofá', '.', 'Ele', 'é', 'muito', 'preguiçoso', '.']\n",
      "Tokenização por sentenças: ['O gato está dormindo no sofá.', 'Ele é muito preguiçoso.']\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Stopwords\n",
    "\n",
    "Stopwords são palavras comuns que são geralmente removidas em análises de texto, pois não contribuem para o significado principal (ex: \"o\", \"e\", \"de\")."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T20:04:47.798494Z",
     "start_time": "2024-09-12T20:04:47.792718Z"
    }
   },
   "source": [
    "# Remover stopwords\n",
    "tokens_sem_stopwords = [token.text for token in doc if not token.is_stop]\n",
    "print(\"Texto sem stopwords:\", tokens_sem_stopwords)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto sem stopwords: ['gato', 'dormindo', 'sofá', '.', 'preguiçoso', '.']\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Lematização\n",
    "\n",
    "A lematização é o processo de transformar uma palavra em sua forma base (lema). O spaCy usa lematização por padrão, pois oferece melhores resultados do que o stemming em muitos casos."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T20:04:51.474713Z",
     "start_time": "2024-09-12T20:04:51.468684Z"
    }
   },
   "source": [
    "# Lematização das palavras no texto\n",
    "lematizadas = [token.lemma_ for token in doc]\n",
    "print(\"Palavras lematizadas:\", lematizadas)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palavras lematizadas: ['o', 'gato', 'estar', 'dormir', 'em o', 'sofá', '.', 'ele', 'ser', 'muito', 'preguiçoso', '.']\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. POS-Tagging (Part-of-Speech Tagging)\n",
    "\n",
    "POS-Tagging é o processo de etiquetar cada palavra em uma sentença com sua respectiva categoria gramatical, como substantivo, verbo, adjetivo, etc."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T20:04:55.258607Z",
     "start_time": "2024-09-12T20:04:55.253129Z"
    }
   },
   "source": [
    "# Aplicar POS-Tagging nas palavras\n",
    "tags = [(token.text, token.pos_) for token in doc]\n",
    "print(\"POS-Tagging:\", tags)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS-Tagging: [('O', 'DET'), ('gato', 'NOUN'), ('está', 'AUX'), ('dormindo', 'VERB'), ('no', 'ADP'), ('sofá', 'NOUN'), ('.', 'PUNCT'), ('Ele', 'PRON'), ('é', 'AUX'), ('muito', 'ADV'), ('preguiçoso', 'ADJ'), ('.', 'PUNCT')]\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. NER (Reconhecimento de Entidades Nomeadas)\n",
    "\n",
    "O NER é o processo de identificar entidades nomeadas, como pessoas, locais, organizações, datas, etc., dentro de um texto."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T20:04:57.961380Z",
     "start_time": "2024-09-12T20:04:57.955788Z"
    }
   },
   "source": [
    "# Aplicar NER no texto\n",
    "entidades = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "print(\"Entidades reconhecidas:\", entidades)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entidades reconhecidas: []\n"
     ]
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
