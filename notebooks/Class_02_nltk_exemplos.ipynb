{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processamento de Linguagem Natural com NLTK\n",
    "\n",
    "Este notebook cobre os tópicos básicos de Processamento de Linguagem Natural (PLN) utilizando a biblioteca NLTK. Incluímos exemplos para:\n",
    "- Tokenização\n",
    "- Stopwords\n",
    "- Stemming\n",
    "- Lematização\n",
    "- POS-Tagging\n",
    "- Reconhecimento de Entidades Nomeadas (NER)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tokenização\n",
    "\n",
    "Tokenização é o processo de dividir o texto em unidades menores chamadas *tokens*. Esses tokens podem ser palavras ou frases."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T19:56:38.424020Z",
     "start_time": "2024-09-12T19:56:37.680113Z"
    }
   },
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# Exemplo de tokenização por palavras\n",
    "texto = \"O gato está dormindo no sofá. Ele é muito preguiçoso.\"\n",
    "tokens_palavras = word_tokenize(texto, language='portuguese')\n",
    "print(\"Tokenização por palavras:\", tokens_palavras)\n",
    "\n",
    "# Exemplo de tokenização por sentenças\n",
    "tokens_sentenças = sent_tokenize(texto, language='portuguese')\n",
    "print(\"Tokenização por sentenças:\", tokens_sentenças)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenização por palavras: ['O', 'gato', 'está', 'dormindo', 'no', 'sofá', '.', 'Ele', 'é', 'muito', 'preguiçoso', '.']\n",
      "Tokenização por sentenças: ['O gato está dormindo no sofá.', 'Ele é muito preguiçoso.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/alex/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Stopwords\n",
    "\n",
    "Stopwords são palavras comuns que são geralmente removidas em análises de texto, pois não contribuem para o significado principal (ex: \"o\", \"e\", \"de\")."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T19:56:50.567125Z",
     "start_time": "2024-09-12T19:56:50.559850Z"
    }
   },
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Carregar lista de stopwords para português\n",
    "stopwords_pt = set(stopwords.words('portuguese'))\n",
    "\n",
    "# Remover stopwords do texto tokenizado\n",
    "tokens_sem_stopwords = [palavra for palavra in tokens_palavras if palavra.lower() not in stopwords_pt]\n",
    "print(\"Texto sem stopwords:\", tokens_sem_stopwords)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto sem stopwords: ['gato', 'dormindo', 'sofá', '.', 'preguiçoso', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/alex/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Stemming\n",
    "\n",
    "Stemming é o processo de reduzir as palavras às suas raízes ou radicais. O algoritmo de stemming remove sufixos e prefixos, deixando apenas a raiz da palavra."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T19:56:59.670929Z",
     "start_time": "2024-09-12T19:56:59.660641Z"
    }
   },
   "source": [
    "from nltk.stem import RSLPStemmer\n",
    "\n",
    "# Inicializar o Stemmer para o português\n",
    "stemmer = RSLPStemmer()\n",
    "\n",
    "# Aplicar stemming nas palavras sem stopwords\n",
    "tokens_com_stemming = [stemmer.stem(palavra) for palavra in tokens_sem_stopwords]\n",
    "print(\"Texto com stemming:\", tokens_com_stemming)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto com stemming: ['gat', 'dorm', 'sof', '.', 'preguiç', '.']\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Lematização\n",
    "\n",
    "A lematização é semelhante ao stemming, mas o objetivo é reduzir a palavra à sua forma base (lema) em vez de uma raiz que pode não ser uma palavra válida."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T19:57:07.233949Z",
     "start_time": "2024-09-12T19:57:04.947767Z"
    }
   },
   "source": [
    "nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Inicializar o lematizador\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Exemplo simples de lematização em inglês (não há suporte nativo completo para português no NLTK)\n",
    "palavras = ['amando', 'correndo', 'pensando']\n",
    "lemmatizadas = [lemmatizer.lemmatize(palavra, pos='v') for palavra in palavras]\n",
    "print(\"Palavras lematizadas:\", lemmatizadas)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/alex/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palavras lematizadas: ['amando', 'correndo', 'pensando']\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. POS-Tagging (Part-of-Speech Tagging)\n",
    "\n",
    "POS-Tagging é o processo de etiquetar cada palavra em uma sentença com sua respectiva categoria gramatical, como substantivo, verbo, adjetivo, etc."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T19:57:38.322422Z",
     "start_time": "2024-09-12T19:57:22.091295Z"
    }
   },
   "source": [
    "nltk.download('mac_morpho')\n",
    "from nltk.corpus import mac_morpho\n",
    "\n",
    "# Treinar um etiquetador com o corpus Mac-Morpho (português)\n",
    "tagger = nltk.UnigramTagger(mac_morpho.tagged_sents())\n",
    "\n",
    "# Aplicar POS-Tagging em uma sentença\n",
    "sentenca = \"O gato está correndo\"\n",
    "tokens_sentenca = word_tokenize(sentenca, language='portuguese')\n",
    "tags = tagger.tag(tokens_sentenca)\n",
    "print(\"POS-Tagging:\", tags)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package mac_morpho to /home/alex/nltk_data...\n",
      "[nltk_data]   Package mac_morpho is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS-Tagging: [('O', 'ART'), ('gato', 'N'), ('está', 'V'), ('correndo', 'V')]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. NER (Reconhecimento de Entidades Nomeadas)\n",
    "\n",
    "O NER é o processo de identificar entidades nomeadas, como pessoas, locais, organizações, datas, etc., dentro de um texto."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T19:57:49.545535Z",
     "start_time": "2024-09-12T19:57:49.140886Z"
    }
   },
   "source": [
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "from nltk import ne_chunk, pos_tag\n",
    "\n",
    "# Exemplo de NER em inglês (não há suporte nativo para NER em português no NLTK)\n",
    "texto_ner = \"Barack Obama was born in Hawaii.\"\n",
    "tokens_ner = word_tokenize(texto_ner)\n",
    "tags_ner = pos_tag(tokens_ner)\n",
    "entidades = ne_chunk(tags_ner)\n",
    "print(\"Entidades reconhecidas:\")\n",
    "print(entidades)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/alex/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/alex/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entidades reconhecidas:\n",
      "(S\n",
      "  (PERSON Barack/NNP)\n",
      "  (PERSON Obama/NNP)\n",
      "  was/VBD\n",
      "  born/VBN\n",
      "  in/IN\n",
      "  (GPE Hawaii/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
