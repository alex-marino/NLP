{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Estrutura do pipeline:\n",
    "- Carregar o dataset: Usar pandas para carregar o dataset data.csv.\n",
    "- Preprocessamento:\n",
    "- Tokenização\n",
    "- Remoção de stopwords\n",
    "- Lematização\n",
    "- Construção da Matriz TF-IDF: Utilizar o TfidfVectorizer da biblioteca scikit-learn para criar a matriz BoW com TF-IDF."
   ],
   "id": "ab2a81c6cbc1bcd6"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-12T20:10:41.954880Z",
     "start_time": "2024-09-12T20:10:06.747209Z"
    }
   },
   "source": [
    "# Instalar as dependências necessárias\n",
    "# !pip install spacy pandas scikit-learn\n",
    "\n",
    "# Baixar o modelo de português\n",
    "# !python -m spacy download pt_core_news_sm\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Carregar o modelo do spaCy para português\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "\n",
    "# Carregar o dataset\n",
    "data = pd.read_csv('../DATA/data.csv')\n",
    "\n",
    "# Exibir as primeiras linhas do dataset\n",
    "print(data.head())\n",
    "\n",
    "# Função de preprocessamento (tokenização, remoção de stopwords, lematização)\n",
    "def preprocessamento(texto):\n",
    "    doc = nlp(texto)  # Processar o texto com spaCy\n",
    "    tokens_limpos = []\n",
    "    \n",
    "    for token in doc:\n",
    "        # Remover stopwords, pontuações e deixar apenas tokens alfabéticos\n",
    "        if not token.is_stop and not token.is_punct and token.is_alpha:\n",
    "            tokens_limpos.append(token.lemma_)  # Lematização\n",
    "    \n",
    "    return ' '.join(tokens_limpos)\n",
    "\n",
    "# Aplicar o preprocessamento a cada sentença no dataset\n",
    "data['processed'] = data['Sentence'].apply(preprocessamento)\n",
    "\n",
    "# Exibir o dataset após o preprocessamento\n",
    "print(data[['Sentence', 'processed']].head())\n",
    "\n",
    "# Criar a matriz TF-IDF usando o texto processado\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Ajustar o vectorizer ao texto pré-processado e transformar\n",
    "X_tfidf = vectorizer.fit_transform(data['processed'])\n",
    "\n",
    "# Exibir a matriz TF-IDF\n",
    "print(\"Matriz TF-IDF (shape):\", X_tfidf.shape)\n",
    "print(X_tfidf.toarray())  # Exibir a matriz BoW TF-IDF como array\n",
    "\n",
    "# Exibir os termos do vocabulário (opcional)\n",
    "print(\"Vocabulário:\", vectorizer.get_feature_names_out())\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Sentence Sentiment\n",
      "0  The GeoSolutions technology will leverage Bene...  positive\n",
      "1  $ESI on lows, down $1.50 to $2.50 BK a real po...  negative\n",
      "2  For the last quarter of 2010 , Componenta 's n...  positive\n",
      "3  According to the Finnish-Russian Chamber of Co...   neutral\n",
      "4  The Swedish buyout firm has sold its remaining...   neutral\n",
      "                                            Sentence  \\\n",
      "0  The GeoSolutions technology will leverage Bene...   \n",
      "1  $ESI on lows, down $1.50 to $2.50 BK a real po...   \n",
      "2  For the last quarter of 2010 , Componenta 's n...   \n",
      "3  According to the Finnish-Russian Chamber of Co...   \n",
      "4  The Swedish buyout firm has sold its remaining...   \n",
      "\n",
      "                                           processed  \n",
      "0  The GeoSolutions technology Will leverage Bene...  \n",
      "1             ESI on low down to BK real possibility  \n",
      "2  the last quarter of Componenta s net sale doub...  \n",
      "3  According to the Chamber of Commerce All the M...  \n",
      "4  The Swedish buyout firm has sold its remaining...  \n",
      "Matriz TF-IDF (shape): (5842, 9888)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Vocabulário: ['aa' 'aal' 'aaland' ... 'àkersberga' 'àland' 'àlandsbanken']\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Explicação do pipeline:\n",
    "# Carregar o dataset:\n",
    "\n",
    "1. Utilizamos o pandas para carregar o arquivo CSV chamado data.csv com as colunas Sentence (contendo o texto) e Sentiment (contendo o sentimento associado).\n",
    "2. O código imprime as primeiras linhas do dataset para verificar o conteúdo carregado.\n",
    "3. Função de preprocessamento:\n",
    "        - A função preprocessamento realiza várias tarefas de limpeza e pré-processamento:\n",
    "            - **Tokenização**: Utilizamos o modelo nlp do spaCy para tokenizar o texto.\n",
    "            - **Remoção de Stopwords**: Remove as stopwords usando o atributo is_stop do spaCy.\n",
    "            - **Remoção de pontuações**: Filtra tokens que são apenas pontuações com is_punct.\n",
    "            - **Lematização**: Usa a lematização para reduzir as palavras à sua forma base.\n",
    "        - Retorna o texto processado como uma string contendo apenas os tokens lematizados e limpos.\n",
    "4. Aplicar preprocessamento:\n",
    "Usamos a função apply do pandas para aplicar o preprocessamento a cada sentença da coluna Sentence, armazenando o resultado em uma nova coluna processed.\n",
    "\n",
    "5. 5.Matriz TF-IDF:\n",
    "Usamos o TfidfVectorizer da biblioteca scikit-learn para criar a matriz TF-IDF a partir dos textos pré-processados.\n",
    "O resultado é uma matriz esparsa onde cada linha representa uma sentença do dataset e cada coluna representa um termo do vocabulário, ponderado pelo TF-IDF.\n",
    "\n",
    "6. 6.Exibir a Matriz TF-IDF:\n",
    "\n",
    "Exibimos o shape da matriz TF-IDF para verificar suas dimensões (número de sentenças x número de termos).\n",
    "Também podemos visualizar a matriz TF-IDF como um array e exibir o vocabulário gerado pelo vectorizador."
   ],
   "id": "f94fc136253366b2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
